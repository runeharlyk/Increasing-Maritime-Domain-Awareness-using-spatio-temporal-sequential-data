{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f83e7e",
   "metadata": {},
   "source": [
    "# Trajectory prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c642b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data.download import download_ais_data, download_file\n",
    "from src.data.cleaning import process_multiple_zip_files\n",
    "from src.data.preprocessing import (\n",
    "    load_and_prepare_data,\n",
    "    create_sequences_with_features,\n",
    "    split_by_vessel,\n",
    "    normalize_data,\n",
    ")\n",
    "from src.models import TrajectoryDataset, EncoderDecoderGRU\n",
    "from src.utils.model_utils import HaversineLoss, train_model, evaluate_model, create_prediction_sequences, predict_trajectories, plot_training_history, load_model_and_config, visualize_predictions\n",
    "from src.visualization import plot_trajectory_comparison, create_prediction_map, create_trajectory_map\n",
    "from src.utils import set_seed\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = Path(\"data\")\n",
    "MODEL_PATH = \"best_model_encoder_decoder.pt\"\n",
    "MODEL = EncoderDecoderGRU\n",
    "INPUT_HOURS = 2\n",
    "OUTPUT_HOURS = 1\n",
    "SAMPLING_RATE = 5\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 3\n",
    "BATCH_SIZE = 2048\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "print(u\"Using device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2fe8e3",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4776b82",
   "metadata": {},
   "source": [
    "### Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3721cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_NAMES = [\n",
    "    \"aisdk-2024-03-01.zip\",\n",
    "    \"aisdk-2024-03-02.zip\",\n",
    "    \"aisdk-2024-03-03.zip\",\n",
    "    # \"aisdk-2024-03-04.zip\",\n",
    "    # \"aisdk-2024-03-05.zip\",\n",
    "    # \"aisdk-2024-03-06.zip\",\n",
    "    # \"aisdk-2024-03-07.zip\",\n",
    "    # \"aisdk-2024-03-08.zip\",\n",
    "    # \"aisdk-2024-03-09.zip\",\n",
    "    # \"aisdk-2024-03-10.zip\"\n",
    "]\n",
    "# ZIP_NAMES = [] // Uncomment to download all files\n",
    "\n",
    "\n",
    "if len(ZIP_NAMES) == 0:\n",
    "    YEAR = \"2024\"\n",
    "    MAX_WORKERS = 8\n",
    "    download_ais_data(YEAR, DATA_DIR, MAX_WORKERS)\n",
    "else:\n",
    "    for ZIP_NAME in ZIP_NAMES:\n",
    "        download_file(\"http://aisdata.ais.dk/2024/\" + ZIP_NAME, DATA_DIR / ZIP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9eaf5f",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ffdc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_multiple_zip_files(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5504ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_prepare_data(DATA_DIR)\n",
    "sequences, targets, mmsi_labels, feature_cols = create_sequences_with_features(\n",
    "    df, INPUT_HOURS, OUTPUT_HOURS, SAMPLING_RATE\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_by_vessel(\n",
    "    sequences, targets, mmsi_labels, train_ratio=0.7, val_ratio=0.15, random_seed=42\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, input_scaler, output_scaler = normalize_data(\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test\n",
    ")\n",
    "\n",
    "train_dataset = TrajectoryDataset(X_train, y_train)\n",
    "val_dataset = TrajectoryDataset(X_val, y_val)\n",
    "test_dataset = TrajectoryDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n",
    "\n",
    "input_size = len(feature_cols)\n",
    "output_timesteps = y_train.shape[1] // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001852ae",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a20e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Unique vessels (MMSI): {df['MMSI'].n_unique()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VESSELS = 100\n",
    "create_trajectory_map(df, MAX_VESSELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3fb2cb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55505dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MODEL(\n",
    "    input_size=input_size,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    output_seq_len=output_timesteps,\n",
    "    dropout=0.3,\n",
    ").to(DEVICE)\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = HaversineLoss(output_scaler).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d6da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nStarting training for {EPOCHS} epochs...\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "early_stop_patience = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    teacher_forcing_ratio = max(0.2, 1.0 - (0.8 * (epoch / EPOCHS)))\n",
    "    train_loss = train_model(model, train_loader, criterion, optimizer, DEVICE, epoch, EPOCHS, teacher_forcing_ratio)\n",
    "    val_loss = evaluate_model(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - \" f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"val_loss\": val_loss,\n",
    "                \"input_scaler\": input_scaler,\n",
    "                \"output_scaler\": output_scaler,\n",
    "                \"config\": {\n",
    "                    \"input_size\": input_size,\n",
    "                    \"hidden_size\": HIDDEN_SIZE,\n",
    "                    \"num_layers\": NUM_LAYERS,\n",
    "                    \"output_seq_len\": output_timesteps,\n",
    "                    \"input_hours\": INPUT_HOURS,\n",
    "                    \"output_hours\": OUTPUT_HOURS,\n",
    "                    \"sampling_rate\": SAMPLING_RATE,\n",
    "                    \"feature_cols\": feature_cols,\n",
    "                },\n",
    "            },\n",
    "            MODEL_PATH,\n",
    "        )\n",
    "        print(f\"  -> Saved best model (val_loss: {val_loss:.6f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nTraining complete! Best validation loss: {best_val_loss:.6f}\")\n",
    "\n",
    "plot_training_history(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a330a3",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(MODEL_PATH, weights_only=False)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "test_loss = evaluate_model(model, test_loader, criterion, DEVICE)\n",
    "print(f\"Final Test Loss: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e546ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, test_loader, output_scaler, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ba976a",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc17b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VESSELS = 200\n",
    "\n",
    "model, config, input_scaler, output_scaler = load_model_and_config(\n",
    "    MODEL_PATH, MODEL\n",
    ")\n",
    "\n",
    "sequences, targets, mmsi_list, full_trajectories, timestamps_list = create_prediction_sequences(\n",
    "    df, config, n_vessels=N_VESSELS\n",
    ")\n",
    "\n",
    "predictions, _ = predict_trajectories(model, sequences, input_scaler, output_scaler)\n",
    "\n",
    "# plot_trajectory_comparison(\n",
    "#     full_trajectories, \n",
    "#     predictions, \n",
    "#     mmsi_list, \n",
    "#     config[\"output_hours\"], \n",
    "# )\n",
    "\n",
    "create_prediction_map(\n",
    "    full_trajectories, \n",
    "    predictions, \n",
    "    mmsi_list, \n",
    "    config[\"output_hours\"], \n",
    "    \"output.html\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
