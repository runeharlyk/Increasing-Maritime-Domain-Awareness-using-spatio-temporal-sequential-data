{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f83e7e",
   "metadata": {},
   "source": [
    "# Trajectory prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c642b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eca682e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from src.data.download import download_ais_data, download_file\n",
    "from src.data.cleaning import process_multiple_zip_files\n",
    "from src.data.preprocessing import (\n",
    "    load_and_prepare_data,\n",
    "    create_sequences,\n",
    "    split_by_vessel,\n",
    "    normalize_data,\n",
    ")\n",
    "\n",
    "from src.models import TrajectoryDataset, EncoderDecoderGRU, EncoderDecoderGRUWithAttention\n",
    "from src.utils.model_utils import HaversineLoss, train_model, evaluate_model, create_prediction_sequences, predict_trajectories, plot_training_history, load_model_and_config, visualize_predictions\n",
    "from src.visualization import plot_trajectory_comparison, create_prediction_map, create_trajectory_map\n",
    "from src.utils import set_seed, haversine_distance\n",
    "from src.utils import pointwise_haversine, mean_haversine_error, rmse_haversine, ade, fde, dtw_distance_trajectory, dtw_batch_mean\n",
    "\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = Path(\"data\")\n",
    "MODEL_PATH = \"best_model_encoder_decoder.pt\"\n",
    "MODEL = EncoderDecoderGRUWithAttention\n",
    "INPUT_HOURS = 2\n",
    "OUTPUT_HOURS = 1\n",
    "SAMPLING_RATE = 5\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 3\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.000001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "print(u\"Using device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2fe8e3",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4776b82",
   "metadata": {},
   "source": [
    "### Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3721cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping aisdk-2024-03-01.zip (already exists)\n"
     ]
    }
   ],
   "source": [
    "ZIP_NAMES = [\n",
    "    \"aisdk-2024-03-01.zip\",\n",
    "    # \"aisdk-2024-03-02.zip\",\n",
    "    # \"aisdk-2024-03-03.zip\",\n",
    "    # \"aisdk-2024-03-04.zip\",\n",
    "    # \"aisdk-2024-03-05.zip\",\n",
    "    # \"aisdk-2024-03-06.zip\",\n",
    "    # \"aisdk-2024-03-07.zip\",\n",
    "    # \"aisdk-2024-03-08.zip\",\n",
    "    # \"aisdk-2024-03-09.zip\",\n",
    "    # \"aisdk-2024-03-10.zip\"\n",
    "]\n",
    "# ZIP_NAMES = [] // Uncomment to download all files\n",
    "\n",
    "if len(ZIP_NAMES) == 0:\n",
    "    YEAR = \"2024\"\n",
    "    MAX_WORKERS = 8\n",
    "    download_ais_data(YEAR, DATA_DIR, MAX_WORKERS)\n",
    "else:\n",
    "    for ZIP_NAME in ZIP_NAMES:\n",
    "        download_file(\"http://aisdata.ais.dk/2024/\" + ZIP_NAME, DATA_DIR / ZIP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9eaf5f",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ffdc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 zip file(s) in data\n",
      "Requested 4 worker(s)\n",
      "Skipping aisdk-2024-03-01.zip - aisdk-2024-03-01.parquet already exists\n",
      "\n",
      "No files to process (all already exist)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'processed': 0, 'skipped': 1, 'failed': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_multiple_zip_files(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f5504ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Found 1 parquet files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 1/1 [00:00<00:00,  6.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3228001 rows from 1 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging continuous segments across file boundaries...\n",
      "  Original (MMSI, FileIndex, Segment) combinations: 821\n",
      "  Merged (MMSI, GlobalSegment) combinations: 821\n",
      "  Merged 0 segment pairs across file boundaries\n",
      "  Time gap threshold: 15.0 minutes\n",
      "\n",
      "Creating sequences (2h input -> 1h output)...\n",
      "  Filtering sequences: min 5 km/h (15 km over 3h)\n",
      "Resampling and adding features with Polars...\n",
      "Creating sequences with stride=1\n",
      "Processing by ['MMSI', 'GlobalSegment'] (merged continuous segments across files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing segments: 100%|██████████| 821/821 [00:02<00:00, 376.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Skipped 7391 sequences with irregular time spacing\n",
      "  Skipped 12345 sequences with insufficient distance traveled\n",
      "Created 65930 sequences from 567 unique vessels\n",
      "  Input shape: (65930, 24, 12)\n",
      "  Target shape: (65930, 24)\n",
      "  Stride: 1 timesteps (5 minutes)\n",
      "  Sequence overlap: ~95.8%\n",
      "\n",
      "Vessel-based split:\n",
      "  Train vessels: 396 (70%)\n",
      "  Val vessels: 85 (15%)\n",
      "  Test vessels: 86 (15%)\n",
      "  Train sequences: 45355\n",
      "  Val sequences: 10437\n",
      "  Test sequences: 10138\n",
      "  ✅ No vessel overlap - proper split confirmed!\n",
      "\n",
      "Normalizing data...\n",
      "  Enforcing uniform spatial scaling: 1.9582\n",
      "  ✅ Data validation passed: No NaNs or Infs detected\n",
      "  X_train_scaled range: [-5.00, 5.00]\n",
      "  y_train_scaled range: [-3.70, 2.64]\n",
      "  Input features: 12\n",
      "  Features normalized (Lat, Lon, SOG, SOG_diff): [0, 1, 2, 9]\n",
      "  Features NOT normalized (sin/cos): [3, 4, 5, 6, 7, 8, 10, 11]\n",
      "  Input scaler - mean: [5.60377200e+01 1.12586038e+01 5.46309012e+00 4.24998081e-03]\n",
      "  Input scaler - scale: [1.95822011 1.95822011 2.21855121 0.75522489]\n",
      "  Output scaler - mean: [56.03325243 11.25642834]\n",
      "  Output scaler - scale: [1.95099968 1.95099968]\n"
     ]
    }
   ],
   "source": [
    "df = load_and_prepare_data(DATA_DIR)\n",
    "sequences, targets, mmsi_labels, feature_cols = create_sequences(\n",
    "    df, INPUT_HOURS, OUTPUT_HOURS, SAMPLING_RATE\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_by_vessel(\n",
    "    sequences, targets, mmsi_labels, train_ratio=0.7, val_ratio=0.15, random_seed=42\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, input_scaler, output_scaler = normalize_data(\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test\n",
    ")\n",
    "\n",
    "train_dataset = TrajectoryDataset(X_train, y_train)\n",
    "val_dataset = TrajectoryDataset(X_val, y_val)\n",
    "test_dataset = TrajectoryDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n",
    "\n",
    "input_size = len(feature_cols)\n",
    "output_timesteps = y_train.shape[1] // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001852ae",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a20e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Unique vessels (MMSI): {df['MMSI'].n_unique()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VESSELS = 100\n",
    "create_trajectory_map(df, MAX_VESSELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3fb2cb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55505dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model architecture:\n",
      "EncoderDecoderGRUWithAttention(\n",
      "  (encoder): GRU(12, 256, num_layers=3, batch_first=True, dropout=0.3)\n",
      "  (attention): Attention(\n",
      "    (W_h): Linear(in_features=256, out_features=256, bias=False)\n",
      "    (W_s): Linear(in_features=256, out_features=256, bias=False)\n",
      "    (v): Linear(in_features=256, out_features=1, bias=False)\n",
      "  )\n",
      "  (decoder): GRU(258, 256, num_layers=3, batch_first=True, dropout=0.3)\n",
      "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 2,314,498\n"
     ]
    }
   ],
   "source": [
    "model = MODEL(\n",
    "    input_size=input_size,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    output_seq_len=output_timesteps,\n",
    "    dropout=0.3,\n",
    ").to(DEVICE)\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "729d1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = HaversineLoss(output_scaler).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d6da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nStarting training for {EPOCHS} epochs...\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "early_stop_patience = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff8ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    teacher_forcing_ratio = max(0.2, 1.0 - (0.8 * (epoch / EPOCHS)))\n",
    "    train_loss = train_model(model, train_loader, criterion, optimizer, DEVICE, epoch, EPOCHS, teacher_forcing_ratio)\n",
    "    val_loss = evaluate_model(model, val_loader, criterion, output_scaler, DEVICE)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - \" f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"val_loss\": val_loss,\n",
    "                \"input_scaler\": input_scaler,\n",
    "                \"output_scaler\": output_scaler,\n",
    "                \"config\": {\n",
    "                    \"input_size\": input_size,\n",
    "                    \"hidden_size\": HIDDEN_SIZE,\n",
    "                    \"num_layers\": NUM_LAYERS,\n",
    "                    \"output_seq_len\": output_timesteps,\n",
    "                    \"input_hours\": INPUT_HOURS,\n",
    "                    \"output_hours\": OUTPUT_HOURS,\n",
    "                    \"sampling_rate\": SAMPLING_RATE,\n",
    "                    \"feature_cols\": feature_cols,\n",
    "                },\n",
    "            },\n",
    "            MODEL_PATH,\n",
    "        )\n",
    "        print(f\"  -> Saved best model (val_loss: {val_loss:.6f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nTraining complete! Best validation loss: {best_val_loss:.6f}\")\n",
    "\n",
    "plot_training_history(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a330a3",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e3a3790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Loss: 3.592035\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(MODEL_PATH, map_location=torch.device(DEVICE), weights_only=False)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "test_loss, test_predictions, true_targets = evaluate_model(model, test_loader, criterion, output_scaler ,DEVICE)\n",
    "print(f\"Final Test Loss: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e546ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, test_loader, output_scaler, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ba976a",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc17b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VESSELS = 25\n",
    "\n",
    "model, config, input_scaler, output_scaler = load_model_and_config(\n",
    "    MODEL_PATH, MODEL\n",
    ")\n",
    "\n",
    "sequences, targets, mmsi_list, full_trajectories, timestamps_list = create_prediction_sequences(\n",
    "    df, config, n_vessels=N_VESSELS\n",
    ")\n",
    "\n",
    "predictions, _ = predict_trajectories(model, sequences, input_scaler, output_scaler)\n",
    "\n",
    "# plot_trajectory_comparison(\n",
    "#     full_trajectories, \n",
    "#     predictions, \n",
    "#     mmsi_list, \n",
    "#     config[\"output_hours\"], \n",
    "# )\n",
    "\n",
    "create_prediction_map(\n",
    "    full_trajectories, \n",
    "    predictions, \n",
    "    mmsi_list, \n",
    "    config[\"output_hours\"], \n",
    "    \"output.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee330952",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "463fbe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true shape: (10138, 12, 2)\n",
      "y_pred shape: (10138, 12, 2)\n",
      "\n",
      "Mean Haversine Error (km): 3.1057066917419434\n",
      "Root Mean Squared Error (km): 4.803907871246338\n",
      "Average Displacement (km): 3.1057066917419434\n",
      "10138\n",
      "Final Displacement (km): 6.41465425491333\n",
      "Dynamic Time Warping Distance (km): 1.4476720768843871\n"
     ]
    }
   ],
   "source": [
    "# Ground truth and predictions, both (N, 2*T)\n",
    "y_pred = test_predictions  # predicted trajectories\n",
    "y_true = true_targets  # ground truth\n",
    "\n",
    "# ---- reshape to (N, T, 2) generically ----\n",
    "N, D = y_true.shape          # N = number of samples, D = 2 * T\n",
    "T = D // 2                   # T = number of timesteps\n",
    "\n",
    "y_true = y_true.reshape(N, T, 2)\n",
    "y_pred = y_pred.reshape(N, T, 2)\n",
    "\n",
    "print(\"y_true shape:\", y_true.shape)  \n",
    "print(\"y_pred shape:\", y_pred.shape)  \n",
    "print()\n",
    "# ---- now metrics will work ----\n",
    "print(\"Mean Haversine Error (km):\", mean_haversine_error(y_true, y_pred))\n",
    "print(\"Root Mean Squared Error (km):\", rmse_haversine(y_true, y_pred))\n",
    "print(\"Average Displacement (km):\", ade(y_true, y_pred))\n",
    "print(\"Final Displacement (km):\", fde(y_true, y_pred))\n",
    "print(\"Dynamic Time Warping Distance (km):\", dtw_batch_mean(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02456deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
