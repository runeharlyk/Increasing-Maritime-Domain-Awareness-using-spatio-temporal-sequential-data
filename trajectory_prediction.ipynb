{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f83e7e",
   "metadata": {},
   "source": [
    "# Trajectory prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c642b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from src.data.download import download_ais_data, download_file\n",
    "from src.data.cleaning import process_multiple_zip_files\n",
    "from src.data.preprocessing import (\n",
    "    load_and_prepare_data,\n",
    "    create_sequences,\n",
    "    split_by_vessel,\n",
    "    normalize_data,\n",
    ")\n",
    "from src.models import TrajectoryDataset, EncoderDecoderGRU, EncoderDecoderGRUWithAttention\n",
    "from src.utils.model_utils import HaversineLoss, train_model, evaluate_model, create_prediction_sequences, predict_trajectories, plot_training_history, load_model_and_config, visualize_predictions\n",
    "from src.visualization import plot_trajectory_comparison, create_prediction_map, create_trajectory_map\n",
    "from src.utils import set_seed, haversine_distance\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = Path(\"data\")\n",
    "MODEL_PATH = \"best_model_encoder_decoder.pt\"\n",
    "MODEL = EncoderDecoderGRUWithAttention\n",
    "INPUT_HOURS = 2\n",
    "OUTPUT_HOURS = 1\n",
    "SAMPLING_RATE = 5\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 3\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.000001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "print(u\"Using device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2fe8e3",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4776b82",
   "metadata": {},
   "source": [
    "### Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3721cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_NAMES = [\n",
    "    \"aisdk-2024-03-01.zip\",\n",
    "    # \"aisdk-2024-03-02.zip\",\n",
    "    # \"aisdk-2024-03-03.zip\",\n",
    "    # \"aisdk-2024-03-04.zip\",\n",
    "    # \"aisdk-2024-03-05.zip\",\n",
    "    # \"aisdk-2024-03-06.zip\",\n",
    "    # \"aisdk-2024-03-07.zip\",\n",
    "    # \"aisdk-2024-03-08.zip\",\n",
    "    # \"aisdk-2024-03-09.zip\",\n",
    "    # \"aisdk-2024-03-10.zip\"\n",
    "]\n",
    "# ZIP_NAMES = [] // Uncomment to download all files\n",
    "\n",
    "\n",
    "if len(ZIP_NAMES) == 0:\n",
    "    YEAR = \"2024\"\n",
    "    MAX_WORKERS = 8\n",
    "    download_ais_data(YEAR, DATA_DIR, MAX_WORKERS)\n",
    "else:\n",
    "    for ZIP_NAME in ZIP_NAMES:\n",
    "        download_file(\"http://aisdata.ais.dk/2024/\" + ZIP_NAME, DATA_DIR / ZIP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9eaf5f",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ffdc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_multiple_zip_files(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5504ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_prepare_data(DATA_DIR)\n",
    "sequences, targets, mmsi_labels, feature_cols = create_sequences(\n",
    "    df, INPUT_HOURS, OUTPUT_HOURS, SAMPLING_RATE\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_by_vessel(\n",
    "    sequences, targets, mmsi_labels, train_ratio=0.7, val_ratio=0.15, random_seed=42\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, input_scaler, output_scaler = normalize_data(\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test\n",
    ")\n",
    "\n",
    "train_dataset = TrajectoryDataset(X_train, y_train)\n",
    "val_dataset = TrajectoryDataset(X_val, y_val)\n",
    "test_dataset = TrajectoryDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n",
    "\n",
    "input_size = len(feature_cols)\n",
    "output_timesteps = y_train.shape[1] // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001852ae",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a20e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Unique vessels (MMSI): {df['MMSI'].n_unique()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VESSELS = 100\n",
    "create_trajectory_map(df, MAX_VESSELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3fb2cb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55505dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MODEL(\n",
    "    input_size=input_size,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    output_seq_len=output_timesteps,\n",
    "    dropout=0.3,\n",
    ").to(DEVICE)\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = HaversineLoss(output_scaler).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d6da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nStarting training for {EPOCHS} epochs...\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "early_stop_patience = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff8ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    teacher_forcing_ratio = max(0.2, 1.0 - (0.8 * (epoch / EPOCHS)))\n",
    "    train_loss = train_model(model, train_loader, criterion, optimizer, DEVICE, epoch, EPOCHS, teacher_forcing_ratio)\n",
    "    val_loss = evaluate_model(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - \" f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"val_loss\": val_loss,\n",
    "                \"input_scaler\": input_scaler,\n",
    "                \"output_scaler\": output_scaler,\n",
    "                \"config\": {\n",
    "                    \"input_size\": input_size,\n",
    "                    \"hidden_size\": HIDDEN_SIZE,\n",
    "                    \"num_layers\": NUM_LAYERS,\n",
    "                    \"output_seq_len\": output_timesteps,\n",
    "                    \"input_hours\": INPUT_HOURS,\n",
    "                    \"output_hours\": OUTPUT_HOURS,\n",
    "                    \"sampling_rate\": SAMPLING_RATE,\n",
    "                    \"feature_cols\": feature_cols,\n",
    "                },\n",
    "            },\n",
    "            MODEL_PATH,\n",
    "        )\n",
    "        print(f\"  -> Saved best model (val_loss: {val_loss:.6f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nTraining complete! Best validation loss: {best_val_loss:.6f}\")\n",
    "\n",
    "plot_training_history(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a330a3",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(MODEL_PATH, map_location=torch.device(DEVICE), weights_only=False)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "test_loss = evaluate_model(model, test_loader, criterion, DEVICE)\n",
    "print(f\"Final Test Loss: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e546ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, test_loader, output_scaler, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ba976a",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc17b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VESSELS = 25\n",
    "\n",
    "model, config, input_scaler, output_scaler = load_model_and_config(\n",
    "    MODEL_PATH, MODEL\n",
    ")\n",
    "\n",
    "sequences, targets, mmsi_list, full_trajectories, timestamps_list = create_prediction_sequences(\n",
    "    df, config, n_vessels=N_VESSELS\n",
    ")\n",
    "\n",
    "predictions, _ = predict_trajectories(model, sequences, input_scaler, output_scaler)\n",
    "\n",
    "# plot_trajectory_comparison(\n",
    "#     full_trajectories, \n",
    "#     predictions, \n",
    "#     mmsi_list, \n",
    "#     config[\"output_hours\"], \n",
    "# )\n",
    "\n",
    "create_prediction_map(\n",
    "    full_trajectories, \n",
    "    predictions, \n",
    "    mmsi_list, \n",
    "    config[\"output_hours\"], \n",
    "    \"output.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee330952",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764f9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_haversine(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute haversine distance at each time step, for each item in batch.\n",
    "\n",
    "    y_true, y_pred: shape (B, T, 2) with [..., 0]=lat_deg, [..., 1]=lon_deg\n",
    "    Returns: distances with shape (B, T) in meters.\n",
    "    \"\"\"\n",
    "\n",
    "    lat_true = y_true[..., 0]\n",
    "    lon_true = y_true[..., 1]\n",
    "    lat_pred = y_pred[..., 0]\n",
    "    lon_pred = y_pred[..., 1]\n",
    "\n",
    "    return haversine_distance(lat_true, lon_true, lat_pred, lon_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff209bcc",
   "metadata": {},
   "source": [
    "## Mean Haversine Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce5f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_haversine_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean haversine error over all batch items and time steps.\n",
    "    \"\"\"\n",
    "    dists = pointwise_haversine(y_true, y_pred)  # (B, T)\n",
    "    return float(dists.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa4ea88",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e5d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_haversine(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    RMSE of haversine distances in meters.\n",
    "    \"\"\"\n",
    "    dists = pointwise_haversine(y_true, y_pred)  # (B, T)\n",
    "    return float(np.sqrt(np.mean(dists ** 2))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c036a7b",
   "metadata": {},
   "source": [
    "## Average Displacement Error (ADE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e3036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ade(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    ADE: Average Displacement Error over whole trajectories.\n",
    "    Here it's effectively the same as mean_haversine_error\n",
    "    \"\"\"\n",
    "    dists = pointwise_haversine(y_true, y_pred)  # (B, T)\n",
    "    return float(dists.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3e92d4",
   "metadata": {},
   "source": [
    "## Final Displacement Error (FDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fde(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    FDE: error at the final time step (averaged over batch).\n",
    "    \"\"\"\n",
    "    dists = pointwise_haversine(y_true, y_pred)  # (B, T)\n",
    "    final_dists = dists[:, -1]  # last time step\n",
    "    return float(final_dists.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f8ca1",
   "metadata": {},
   "source": [
    "## Dynamic Time Warping Distance (DTW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtw_distance_trajectory(traj_true, traj_pred):\n",
    "    \"\"\"\n",
    "    DTW distance between two single trajectories.\n",
    "\n",
    "    traj_true, traj_pred: shape (T, 2) with [lat_deg, lon_deg]\n",
    "    Returns: DTW distance in meters (normalized by path length).\n",
    "    \"\"\"\n",
    "\n",
    "    T1 = traj_true.shape[0]\n",
    "    T2 = traj_pred.shape[0]\n",
    "\n",
    "    # Cost matrix: pairwise haversine distances\n",
    "    # Shape (T1, T2)\n",
    "    lat1 = traj_true[:, 0][:, None]  # (T1, 1)\n",
    "    lon1 = traj_true[:, 1][:, None]  # (T1, 1)\n",
    "    lat2 = traj_pred[:, 0][None, :]  # (1, T2)\n",
    "    lon2 = traj_pred[:, 1][None, :]  # (1, T2)\n",
    "\n",
    "    cost = haversine_distance(lat1, lon1, lat2, lon2)\n",
    "\n",
    "    # Accumulated cost matrix\n",
    "    acc = np.zeros((T1, T2), dtype=np.float64)\n",
    "    acc[0, 0] = cost[0, 0]\n",
    "\n",
    "    # First row\n",
    "    for j in range(1, T2):\n",
    "        acc[0, j] = cost[0, j] + acc[0, j - 1]\n",
    "\n",
    "    # First column\n",
    "    for i in range(1, T1):\n",
    "        acc[i, 0] = cost[i, 0] + acc[i - 1, 0]\n",
    "\n",
    "    # Rest\n",
    "    for i in range(1, T1):\n",
    "        for j in range(1, T2):\n",
    "            acc[i, j] = cost[i, j] + min(\n",
    "                acc[i - 1, j],      # insertion\n",
    "                acc[i, j - 1],      # deletion\n",
    "                acc[i - 1, j - 1],  # match\n",
    "            )\n",
    "\n",
    "    # Option 1: raw DTW distance\n",
    "    # return float(acc[-1, -1])\n",
    "\n",
    "    # Option 2: normalized by path length (more comparable)\n",
    "    path_length = T1 + T2  # rough normalization\n",
    "    return float(acc[-1, -1] / path_length)\n",
    "\n",
    "\n",
    "def dtw_batch_mean(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean DTW distance over a batch of trajectories.\n",
    "\n",
    "    y_true, y_pred: shape (B, T, 2)\n",
    "    Returns: scalar, average DTW distance in meters.\n",
    "    \"\"\"\n",
    "\n",
    "    B = y_true.shape[0]\n",
    "    dtw_values = []\n",
    "\n",
    "    for b in range(B):\n",
    "        dtw_b = dtw_distance_trajectory(y_true[b], y_pred[b])\n",
    "        dtw_values.append(dtw_b)\n",
    "\n",
    "    return float(np.mean(dtw_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ee756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_true = np.array([\n",
    "    [[55.0, 12.0], [55.001, 12.002], [55.002, 12.004], [55.003, 12.006], [55.004, 12.008]],\n",
    "    [[56.0, 10.0], [56.001, 10.002], [56.002, 10.004], [56.003, 10.006], [56.004, 10.008]],\n",
    "])\n",
    "\n",
    "# Slightly perturbed predictions\n",
    "y_pred = y_true.copy()\n",
    "y_pred += np.random.normal(scale=0.0005, size=y_true.shape)  # small noise\n",
    "\n",
    "print(\"Mean Haversine Error (km):\", mean_haversine_error(y_true, y_pred))\n",
    "print(\"Root Mean Squared Error (km):      \", rmse_haversine(y_true, y_pred))\n",
    "print(\"Average Displacement (km):                 \", ade(y_true, y_pred))\n",
    "print(\"Final Displacement (km):                 \", fde(y_true, y_pred))\n",
    "print(\"Dynamic Time Warping Distance (km):            \", dtw_batch_mean(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e17620a",
   "metadata": {},
   "source": [
    "1. Mean Haversine Error\n",
    "\n",
    "Overall accuracy. Shows average geo prediction error in meters.\n",
    "\n",
    "2. RMSE\n",
    "\n",
    "Penalizes the larger mistakes more. Helps detect if the model makes big errors.\n",
    "\n",
    "3. ADE (Average Displacement Error)\n",
    "\n",
    "Accuracy over entire predicted trajectory. SHows how model follows true path in general.\n",
    "\n",
    "4. FDE (Final Displacement Error)\n",
    "\n",
    "How far it is from final position. Sometimes error grow over time.\n",
    "\n",
    "5. DTW (Dynamic Time Warping)\n",
    "\n",
    "Compares shape of predicted and real traj. Used to see if model captures realistic movement patt.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02456deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
