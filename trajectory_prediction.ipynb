{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f83e7e",
   "metadata": {},
   "source": [
    "# Trajectory prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c642b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import platform\n",
    "\n",
    "from src.data.download import download_ais_data, download_file\n",
    "from src.data.cleaning import process_multiple_zip_files\n",
    "from src.data.preprocessing import (\n",
    "    load_and_prepare_data,\n",
    "    create_sequences,\n",
    "    split_by_vessel,\n",
    "    normalize_data,\n",
    ")\n",
    "\n",
    "from src.models import TrajectoryDataset, EncoderDecoderGRU, EncoderDecoderGRUWithAttention\n",
    "from src.utils.model_utils import HaversineLoss, LandMaskHaversineLoss, train_model, evaluate_model, create_prediction_sequences, predict_trajectories, plot_training_history, load_model_and_config, visualize_predictions\n",
    "from src.visualization import create_prediction_map, create_trajectory_map\n",
    "from src.utils import set_seed, pointwise_haversine, mean_haversine_error, rmse_haversine, fde, dtw_distance_trajectory, dtw_batch_mean, LandMask, config\n",
    "\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = Path(\"data\")\n",
    "MODELS = [[EncoderDecoderGRU, \"best_model_encoder_decoder.pt\"], [EncoderDecoderGRUWithAttention, \"best_model_encoder_decoder_with_attention.pt\"]]\n",
    "LOSS_FUNCTION = LandMaskHaversineLoss\n",
    "INPUT_HOURS = 2\n",
    "OUTPUT_HOURS = 1\n",
    "SAMPLING_RATE = 5\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 3\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "LAND_WEIGHT = 5.0\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() \n",
    "    else \"mps\" if torch.backends.mps.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "NUM_WORKERS = 0 if platform.system() == \"Windows\" else 8\n",
    "SEED = 42\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Platform: {platform.system()}\")\n",
    "print(f\"DataLoader workers: {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2fe8e3",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4776b82",
   "metadata": {},
   "source": [
    "### Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3721cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_NAMES = [\n",
    "    \"aisdk-2024-03-01.zip\",\n",
    "    # \"aisdk-2024-03-02.zip\",\n",
    "    # \"aisdk-2024-03-03.zip\",\n",
    "    # \"aisdk-2024-03-04.zip\",\n",
    "    # \"aisdk-2024-03-05.zip\",\n",
    "    # \"aisdk-2024-03-06.zip\",\n",
    "    # \"aisdk-2024-03-07.zip\",\n",
    "    # \"aisdk-2024-03-08.zip\",\n",
    "    # \"aisdk-2024-03-09.zip\",\n",
    "    # \"aisdk-2024-03-10.zip\"\n",
    "]\n",
    "# ZIP_NAMES = [] // Uncomment to download all files\n",
    "\n",
    "if len(ZIP_NAMES) == 0:\n",
    "    YEAR = \"2024\"\n",
    "    MAX_WORKERS = 8\n",
    "    download_ais_data(YEAR, DATA_DIR, MAX_WORKERS)\n",
    "else:\n",
    "    for ZIP_NAME in ZIP_NAMES:\n",
    "        download_file(\"http://aisdata.ais.dk/2024/\" + ZIP_NAME, DATA_DIR / ZIP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9eaf5f",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ffdc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_multiple_zip_files(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5504ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_prepare_data(DATA_DIR)\n",
    "sequences, targets, mmsi_labels, feature_cols = create_sequences(\n",
    "    df, INPUT_HOURS, OUTPUT_HOURS, SAMPLING_RATE\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_by_vessel(\n",
    "    sequences, targets, mmsi_labels, train_ratio=0.7, val_ratio=0.15, random_seed=42\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, input_scaler, output_scaler = normalize_data(\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test\n",
    ")\n",
    "\n",
    "train_dataset = TrajectoryDataset(X_train, y_train)\n",
    "val_dataset = TrajectoryDataset(X_val, y_val)\n",
    "test_dataset = TrajectoryDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "input_size = len(feature_cols)\n",
    "output_timesteps = y_train.shape[1] // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001852ae",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a20e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Unique vessels (MMSI): {df['MMSI'].n_unique()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VESSELS = 100\n",
    "create_trajectory_map(df, MAX_VESSELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3fb2cb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55505dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [ModelClass(\n",
    "    input_size=input_size,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    output_seq_len=output_timesteps,\n",
    "    dropout=0.3,\n",
    ").to(DEVICE) for ModelClass, _ in MODELS]\n",
    "\n",
    "print(\"\\nModels architecture:\")\n",
    "for i, m in enumerate(models):\n",
    "    print(f\"\\nModel #{i}:\")\n",
    "    print(m)\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in m.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOSS_FUNCTION == LandMaskHaversineLoss:\n",
    "    land_mask = LandMask(bounds=config.BOUNDING_BOX)\n",
    "    criterion = LOSS_FUNCTION(\n",
    "        output_scaler,\n",
    "        land_mask=land_mask,\n",
    "        land_weight=LAND_WEIGHT,\n",
    "    ).to(DEVICE)\n",
    "else:\n",
    "    criterion = LOSS_FUNCTION(output_scaler).to(DEVICE)\n",
    "early_stop_patience = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff8ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_loop(model, model_path):    \n",
    "    # reset things for this model\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.5, patience=10\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    print(f\"\\nTraining model {type(model).__name__} for {EPOCHS} epochs...\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        teacher_forcing_ratio = max(0.2, 1.0 - (0.8 * (epoch / EPOCHS)))\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer, DEVICE, epoch, EPOCHS, teacher_forcing_ratio)\n",
    "        val_loss = evaluate_model(model, val_loader, criterion, output_scaler, DEVICE)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] - \" f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"input_scaler\": input_scaler,\n",
    "                    \"output_scaler\": output_scaler,\n",
    "                    \"config\": {\n",
    "                        \"input_size\": input_size,\n",
    "                        \"hidden_size\": HIDDEN_SIZE,\n",
    "                        \"num_layers\": NUM_LAYERS,\n",
    "                        \"output_seq_len\": output_timesteps,\n",
    "                        \"input_hours\": INPUT_HOURS,\n",
    "                        \"output_hours\": OUTPUT_HOURS,\n",
    "                        \"sampling_rate\": SAMPLING_RATE,\n",
    "                        \"feature_cols\": feature_cols,\n",
    "                    },\n",
    "                },\n",
    "                model_path,\n",
    "            )\n",
    "            print(f\"  -> Saved best model (val_loss: {val_loss:.6f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stop_patience:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nFinished training model: {type(model).__name__}\")\n",
    "    print(f\"\\nTraining complete! Best validation loss: {best_val_loss:.6f}\")\n",
    "\n",
    "    plot_training_history(train_losses, val_losses)\n",
    "\n",
    "for model, (_, model_path) in zip(models, MODELS):\n",
    "    train_model_loop(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a330a3",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_loop(model, model_path):   \n",
    "    checkpoint = torch.load(model_path, map_location=torch.device(DEVICE), weights_only=False)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    test_loss, test_predictions, true_targets = evaluate_model(model, test_loader, criterion, output_scaler, DEVICE, return_predictions=True)\n",
    "    print(f\"Final Test Loss for {type(model).__name__}: {test_loss:.6f}\")\n",
    "    return test_predictions, true_targets\n",
    "    \n",
    "test_predictions = []\n",
    "true_targets = []\n",
    "\n",
    "for model, (_, model_path) in zip(models, MODELS):\n",
    "    tp, tt = test_model_loop(model, model_path)\n",
    "    test_predictions.append(tp)\n",
    "    true_targets.append(tt)\n",
    "\n",
    "test_predictions = np.asarray(test_predictions)\n",
    "true_targets = np.asarray(true_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e546ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    visualize_predictions(model, test_loader, output_scaler, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ba976a",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc17b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VESSELS = 250\n",
    "\n",
    "def final_predictions(ModelClass, model_path):\n",
    "    model, config, input_scaler, output_scaler = load_model_and_config(\n",
    "        model_path, ModelClass, DEVICE\n",
    "    )\n",
    "\n",
    "    sequences, targets, mmsi_list, full_trajectories, timestamps_list = create_prediction_sequences(\n",
    "        df, config, n_vessels=N_VESSELS\n",
    "    )\n",
    "\n",
    "    predictions, _ = predict_trajectories(model, sequences, input_scaler, output_scaler, DEVICE)\n",
    "\n",
    "    output_filename = f\"predictions_{ModelClass.__name__}.html\"\n",
    "    \n",
    "    create_prediction_map(\n",
    "        full_trajectories, \n",
    "        predictions, \n",
    "        mmsi_list, \n",
    "        config[\"output_hours\"], \n",
    "        output_filename\n",
    "    )\n",
    "    \n",
    "    print(f\"Saved predictions map to {output_filename}\")\n",
    "    \n",
    "for ModelClass, model_path in MODELS:\n",
    "    final_predictions(ModelClass, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee330952",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "463fbe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: EncoderDecoderGRU\n",
      "y_true shape: (24640, 12, 2)\n",
      "y_pred shape: (24640, 12, 2)\n",
      "\n",
      "Mean Haversine Error (km): 1.294181227684021\n",
      "Root Mean Squared Error (km): 2.4115805625915527\n",
      "Final Displacement (km): 2.6539855003356934\n",
      "Dynamic Time Warping Distance (km): 0.6225341429800272\n",
      "\n",
      "\n",
      "\n",
      "Model: EncoderDecoderGRUWithAttention\n",
      "y_true shape: (24640, 12, 2)\n",
      "y_pred shape: (24640, 12, 2)\n",
      "\n",
      "Mean Haversine Error (km): 1.2427228689193726\n",
      "Root Mean Squared Error (km): 2.3339483737945557\n",
      "Final Displacement (km): 2.507740020751953\n",
      "Dynamic Time Warping Distance (km): 0.5960654776851654\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model, tp, tt in zip(models, test_predictions, true_targets):\n",
    "    print(f\"Model: {type(model).__name__}\")\n",
    "    \n",
    "    # Ground truth and predictions, both (N, 2*T)\n",
    "    y_pred = tp  # predicted trajectories\n",
    "    y_true = tt  # ground truth\n",
    "\n",
    "    # ---- reshape to (N, T, 2) generically ----\n",
    "    N, D = y_true.shape          # N = number of samples, D = 2 * T\n",
    "    T = D // 2                   # T = number of timesteps\n",
    "\n",
    "    y_true = y_true.reshape(N, T, 2)\n",
    "    y_pred = y_pred.reshape(N, T, 2)\n",
    "\n",
    "    print(\"y_true shape:\", y_true.shape)  \n",
    "    print(\"y_pred shape:\", y_pred.shape) \n",
    "    print()\n",
    "    # ---- now metrics will work ----\n",
    "    print(\"Mean Haversine Error (km):\", mean_haversine_error(y_true, y_pred))\n",
    "    print(\"Root Mean Squared Error (km):\", rmse_haversine(y_true, y_pred))\n",
    "    print(\"Final Displacement (km):\", fde(y_true, y_pred))\n",
    "    print(\"Dynamic Time Warping Distance (km):\", dtw_batch_mean(y_true, y_pred))\n",
    "    print(\"\\n\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02456deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
